{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jérémy TREMBLAY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 : Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries that will be used in this notebook.\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Import the pyplot module from matplotlib with the plt alias.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Other usefull libraries\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Create a `Bandit` class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne :** Créer une représentation d’une machine bandit dans une classe nommée `Bandit` :  \n",
    "* Qui prend en paramètre la moyenne et l’écart-type d’une distribution gaussienne.\n",
    "* Définir la méthode `draw` qui permet de tirer le bras du bandit et de retourner un gain potentiel.  \n",
    "\n",
    "Ce gain sera directement fournit par la distribution et ses paramètres."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    \"\"\"Bandit class. Represents a bandit in a Casino game.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std_dev):\n",
    "        \"\"\"\n",
    "        Bandit constructor.\n",
    "\n",
    "        Args:\n",
    "            mean: {float} -- The mean of the Gaussian distribution.\n",
    "            std_dev: {float} -- The standart deviation of the Gaussian distribution.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_dev\n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Simulate pulling the bandit's arm and return a potential gain.\n",
    "\n",
    "        Returns:\n",
    "            {float} -- The potential gani drawn from the Gaussian distribution.\n",
    "        \"\"\"\n",
    "        return np.random.normal(self.mean, self.std_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Bandit and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39.8659070721706\n",
      "10.032084283078067\n",
      "-23.387847901037027\n"
     ]
    }
   ],
   "source": [
    "bandit_test = Bandit(20, 40)\n",
    "print(bandit_test.draw())\n",
    "print(bandit_test.draw())\n",
    "print(bandit_test.draw())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok it seems coherent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create a `Casino` class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne :** Créer une représentation d’un casino, à partir d’une classe `Casino` :  \n",
    "* Un casino est composée de plusieurs machines `bandits`.\n",
    "* Définir la méthode `play` qui permet de tirer le bras d’un bandit ciblé par son index et retourne son gain si le bandit est bien disponible.\n",
    "* Créer une méthode de type *property*, qui permet de retourner le nombre de `bandits` présents dans le casino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Casino:\n",
    "    \"\"\"Casino class, represents a Casino where players can play games (and lose money).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Casino constructor.\n",
    "\n",
    "        Initializes an empty lsit to store bandits.\n",
    "        \"\"\"\n",
    "        self.bandits = []\n",
    "\n",
    "    def __init__(self, bandits):\n",
    "        \"\"\"\n",
    "        Casino constructor.\n",
    "\n",
    "        Initializes a list to store bandits.\n",
    "        \"\"\"\n",
    "        self.bandits = []\n",
    "        [self.bandits.append(bandit) for bandit in bandits]\n",
    "\n",
    "    def play(self, index):\n",
    "        \"\"\"\n",
    "        Simulate playing a specific bandit in the casino.\n",
    "\n",
    "        Args:\n",
    "            bandit_index: {int} -- Th index of the bandit to be played.\n",
    "\n",
    "        Returns:\n",
    "            {float} -- The gain obtained from playng the selected bandit.\n",
    "        \"\"\"\n",
    "        if index >= 0 and index < len(self.bandits):\n",
    "            return self.bandits[index].draw()\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def number_of_bandits(self):\n",
    "        \"\"\"\n",
    "        Get the number of bandits in the casino.\n",
    "\n",
    "        Returns:\n",
    "            {int} -- The number of bandits in the casino.\n",
    "        \"\"\"\n",
    "        return len(self.bandits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the Casino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "31.051435491285524\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "casino_test = Casino([bandit_test])\n",
    "print(casino_test.play(-1))\n",
    "print(casino_test.play(0))\n",
    "print(casino_test.play(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create an `Agent` class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne :** Créer une classe abstraite `Agent` telle que :\n",
    "* Elle est associée à une instance de casino.\n",
    "* Enregistre les gains obtenus pour chaque machine bandit présent dans le casino.\n",
    "* Permet de connaître le nombre de tour de jeu effectué.\n",
    "* Permet l’accès à une propriété `rewards`, qui fournit la somme des récompenses de l’agent.\n",
    "* Une méthode abstraite privée `_policy` qui détermine le bandit sélectionné pour un tour de jeu (politique de choix). Elle retourne l’index de ce bandit.\n",
    "* Une méthode `select` générique qui permet d’activer la politique de choix du bandit, de jouer ce bandit et de mettre à jour les récompenses reçues pour ce bandit.\n",
    "* Une méthode de surcharge `__str__` qui permet un affichage simple de la classe : type actuel de la classe, informations sur les bandits et gain total (vous pouvez revenir sur son développement par la suite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    \"\"\"Agent class. Represents a worker that make some manipulations in the casino.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, casino):\n",
    "        \"\"\"\n",
    "        Agent constructor.\n",
    "\n",
    "        Args:\n",
    "            casino: {Casino} -- The Casino object associated with the agent.\n",
    "        \"\"\"\n",
    "        self.casino = casino\n",
    "        self.gains = [0 for _ in range(casino.number_of_bandits)]\n",
    "        self.number_of_turns = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def _policy(self):\n",
    "        \"\"\"\n",
    "        Abstract method to determine the bandit selected for a round of play.\n",
    "\n",
    "        Returns:\n",
    "            {int} -- The index of the selected bandit.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def select(self):\n",
    "        \"\"\"\n",
    "        Activate the bandit selection policy, play the selected bandit, and update rewarsd.\n",
    "\n",
    "        Returns:\n",
    "            {float} -- The gain obtained from playing the selected bandit.\n",
    "        \"\"\"\n",
    "        bandit_index = self._policy()\n",
    "        reward = self.casino.play(bandit_index)\n",
    "        self.gains[bandit_index] += reward\n",
    "        self.number_of_turns += 1\n",
    "        return reward\n",
    "\n",
    "    @property\n",
    "    def rewards(self):\n",
    "        \"\"\"\n",
    "        Get the number of rewards at the total for the casino.\n",
    "\n",
    "        Returns:\n",
    "            {int} -- The gains made in the casino by all the bandits.\n",
    "        \"\"\"\n",
    "        return sum(self.gains)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        String representation of the Agent class.\n",
    "\n",
    "        Returns:\n",
    "            {str} -- The string representation of the Agent class.\n",
    "        \"\"\"\n",
    "        string = f\"{type(self).__name__}:\\n\"\n",
    "        for i in range(self.casino.number_of_bandits):\n",
    "            string += f\"- {i}: {self.gains[i]:0.3f}\\n\"\n",
    "        return f\"{string}Total: {self.rewards} in {self.number_of_turns} rounds.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create a greedy agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne :** À partir de cette représentation abstraite de votre agent, créer l’agent `Greedy` (Glouton), qui aura comme politique de choix d’action :  \n",
    "* Tester une fois chaque machine présente dans le casino.\n",
    "* Puis, de toujours sélectionner celle qui lui avait fournie le meilleur gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Greedy(Agent):\n",
    "    def __init(self, casino):\n",
    "        \"\"\"\n",
    "        GreedyAgent constructor.\n",
    "\n",
    "        Initializes the GreedyAgent with the casino.\n",
    "\n",
    "        Args:\n",
    "            casino: {Casino} -- The Casino object associated with the agent.\n",
    "        \"\"\"\n",
    "        super().__init__(casino)\n",
    "\n",
    "    def _policy(self):\n",
    "        \"\"\"\n",
    "        Select the bandit based on the greedy policy (always the one who produced the best score once a turn for each).\n",
    "\n",
    "        Returns:\n",
    "            {int} -- The index of the selected bandit.\n",
    "        \"\"\"\n",
    "        if self.number_of_turns < self.casino.number_of_bandits:\n",
    "            return self.number_of_turns\n",
    "        else:\n",
    "            return np.argmax(self.gains)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy:\n",
      "- 0: 9972.260\n",
      "- 1: 1.268\n",
      "- 2: 2.293\n",
      "Total: 9975.821706919874 in 1000 rounds.\n"
     ]
    }
   ],
   "source": [
    "# Create a Casino with three bandits and the agent.\n",
    "casino = Casino([Bandit(mean=10, std_dev=1), Bandit(mean=5, std_dev=2), Bandit(mean=2, std_dev=0.5)])\n",
    "greedy_agent = Greedy(casino)\n",
    "\n",
    "# Make the agent play for a certain number of rounds.\n",
    "num_rounds_to_play = 1000\n",
    "for _ in range(num_rounds_to_play):\n",
    "    greedy_agent.select()\n",
    "\n",
    "# Print the final results.\n",
    "print(greedy_agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create an epsilon greedy agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne :** Créer maintenant l’agent `EpsilonGreedy`, qui aura comme politique de choix d’action :\n",
    "* Tester une fois chaque machine présente dans le casino.\n",
    "* Puis, de toujours sélectionner celle qui lui avait fournie le meilleur gain sauf sous contrainte d’un critère probabiliste *ϵ* € [0, 1], de choisir une action aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy(Agent):\n",
    "    def __init__(self, casino, epsilon):\n",
    "        \"\"\"\n",
    "        EpsilonGreedy constructor.\n",
    "\n",
    "        Initializes the EpsilonGreedy agetn with the casino and epsilon.\n",
    "\n",
    "        Args:\n",
    "            casino: {Casino} -- The Casino object associated with the agent.\n",
    "            epsilon: {float} -- The exploration-exploitation trade-off parameter.\n",
    "        \"\"\"\n",
    "        super().__init__(casino)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _policy(self):\n",
    "        \"\"\"\n",
    "        Select the bandit based on the epsilon-greedy policy.\n",
    "\n",
    "        Returns:\n",
    "            {int} -- The index of the selected bandit.\n",
    "        \"\"\"\n",
    "        if self.number_of_turns < self.casino.number_of_bandits:\n",
    "            return self.number_of_turns\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                # Explore: Choose a random bandit.\n",
    "                bandit_index = np.random.randint(self.casino.number_of_bandits)\n",
    "            else:\n",
    "                # Exploit: Choose the bandit with the highest total rewards.\n",
    "                bandit_index = np.argmax(self.rewards)\n",
    "\n",
    "        return bandit_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpsilonGreedy:\n",
      "- 0: 927.305\n",
      "- 1: 80.120\n",
      "- 2: 16.565\n",
      "Total: 1023.9907817765684 in 1000 rounds.\n"
     ]
    }
   ],
   "source": [
    "# Create a casino with bandits.\n",
    "bandit1 = Bandit(1.0, 0.1)\n",
    "bandit2 = Bandit(2.0, 0.5)\n",
    "bandit3 = Bandit(0.5, 0.2)\n",
    "\n",
    "casino = Casino([bandit1, bandit2, bandit3])\n",
    "\n",
    "# Create an EpsilonGreedy agent with an epsilon value of 0.1.\n",
    "epsilon_greedy_agent = EpsilonGreedy(casino, epsilon=0.1)\n",
    "\n",
    "# Run the agent.\n",
    "for _ in range(1000):\n",
    "    epsilon_greedy_agent.select()\n",
    "\n",
    "# Display results.\n",
    "print(epsilon_greedy_agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see heere that it has sometimes test another bandit than the one that is best suitable for him, because of the epsilon parameter. More we increase it, more we can observe an average for the bandits used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Define a casino context and simulate agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne :** Définir un contexte de casino afin de simuler vos stratégies d’agents :\n",
    "* Définir 4 bandits ayant respectivement les distributions suivantes : N(2, 3), N(-1, 6), N(-2, 3), N(5, 3).\n",
    "* Simuler pour chaque action les gains potentiels obtenus par vos agents sur 1000 tours (nombre de sélections d’action).\n",
    "* Analysez les résultats obtenues en faisant varier le paramètre *ϵ* de votre agent de type `EpsilonGreedy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Greedy agent:\n",
      "Greedy:\n",
      "- 0: 0.547\n",
      "- 1: -10.478\n",
      "- 2: -3.496\n",
      "- 3: 4933.841\n",
      "Total: 4920.412933976326 in 1000 rounds.\n",
      "\n",
      "Results for EpsilonGreedy agent with epsilon=0.1:\n",
      "EpsilonGreedy:\n",
      "- 0: 1551.618\n",
      "- 1: -46.660\n",
      "- 2: -85.496\n",
      "- 3: 144.402\n",
      "Total: 1563.865059954677 in 1000 rounds.\n",
      "\n",
      "Results for EpsilonGreedy agent with epsilon=0.2:\n",
      "EpsilonGreedy:\n",
      "- 0: 1782.916\n",
      "- 1: -33.997\n",
      "- 2: -78.843\n",
      "- 3: 229.742\n",
      "Total: 1899.8178287643966 in 1000 rounds.\n",
      "\n",
      "Results for EpsilonGreedy agent with epsilon=0.4:\n",
      "EpsilonGreedy:\n",
      "- 0: 1337.505\n",
      "- 1: -24.146\n",
      "- 2: -171.253\n",
      "- 3: 519.266\n",
      "Total: 1661.3730264593119 in 1000 rounds.\n",
      "\n",
      "Results for EpsilonGreedy agent with epsilon=0.5:\n",
      "EpsilonGreedy:\n",
      "- 0: 1321.879\n",
      "- 1: -119.012\n",
      "- 2: -271.498\n",
      "- 3: 560.954\n",
      "Total: 1492.3229348004961 in 1000 rounds.\n",
      "\n",
      "Results for EpsilonGreedy agent with epsilon=0.6:\n",
      "EpsilonGreedy:\n",
      "- 0: 989.963\n",
      "- 1: -59.126\n",
      "- 2: -282.208\n",
      "- 3: 867.384\n",
      "Total: 1516.013354117556 in 1000 rounds.\n",
      "\n",
      "Results for EpsilonGreedy agent with epsilon=0.8:\n",
      "EpsilonGreedy:\n",
      "- 0: 633.898\n",
      "- 1: -145.966\n",
      "- 2: -352.818\n",
      "- 3: 1109.136\n",
      "Total: 1244.2504798460118 in 1000 rounds.\n",
      "\n",
      "Results for EpsilonGreedy agent with epsilon=1:\n",
      "EpsilonGreedy:\n",
      "- 0: 495.545\n",
      "- 1: -280.121\n",
      "- 2: -534.478\n",
      "- 3: 1236.560\n",
      "Total: 917.5055429012193 in 1000 rounds.\n"
     ]
    }
   ],
   "source": [
    "# First let's define the bandits.\n",
    "bandit1 = Bandit(2, 3)\n",
    "bandit2 = Bandit(-1, 6)\n",
    "bandit3 = Bandit(-2, 3)\n",
    "bandit4 = Bandit(5, 3)\n",
    "\n",
    "# Create the casino and the agent.\n",
    "casino = Casino([bandit1, bandit2, bandit3, bandit4])\n",
    "greedy_agent = Greedy(casino)\n",
    "total_turns = 1000\n",
    "\n",
    "# Let's simulate the agent.\n",
    "for _ in range(total_turns):\n",
    "    greedy_agent.select()\n",
    "\n",
    "# Display Greedy agent's results.\n",
    "print(\"Results for Greedy agent:\")\n",
    "print(greedy_agent)\n",
    "\n",
    "# Simulate strategies for tje EpsilonGreedy greedy agent with different values for epsilon.\n",
    "epsilon_values = [0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 1]\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    epsilon_greedy_agent = EpsilonGreedy(casino, epsilon)\n",
    "    for _ in range(total_turns):\n",
    "        epsilon_greedy_agent.select()\n",
    "\n",
    "    # Display EpsilonGreedy agent's results.\n",
    "    print(f\"\\nResults for EpsilonGreedy agent with epsilon={epsilon}:\")\n",
    "    print(epsilon_greedy_agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Greedy agent consistently favors the bandit with the highest total reward, leading to significant gains from the bandit with mean 5 and standard deviation 3.\n",
    "\n",
    "On the other hand, the EpsilonGreedy agent explores different bandits based on the epsilon parameter, balancing exploration and exploitatino. As epsilon increases, the agent becomes more explorative, resulting in a trade-off between discovering potentially better bandits and exploiting the currently best knonw bandit. This trade-off is reflected in the total rewards, with higher epsilon values leading to more exploration and potentially lower overall gains.\n",
    "\n",
    "In summary, adjusting the epsilon parameter allows fine-tuning the exploration-exploitation strategy of the agent. Higher epsilon values promote exploration, providing a balance between exploiting the best-known bandit and discovering potentially better alternatives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Create an UCB agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne :** La formule de l'UCB est la suivante :\n",
    "\n",
    "`UCB = Ri + c x sqrt(log(N) / ni)`\n",
    "\n",
    "Développer l’agent UCB afin de le comparer comme précédemment, tel que :\n",
    "* Comme pour les autres stratégies, chaque machine est explorée une fois.\n",
    "* Puis, la stratégie de choix des actions (politique) est maintenant basée sur la formule UCB.\n",
    "* L’action finalement sélectionnée est celle fournissant la valeur maximale UCB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB_Agent(Agent):\n",
    "    \"\"\"UCB_Agent class. Represents an agent using the UCB strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, casino, c):\n",
    "        \"\"\"\n",
    "        UCB_Agent constructor.\n",
    "\n",
    "        Args:\n",
    "            casino: {Casino} -- The Casino object associated with the agent.\n",
    "            c: {float} -- The exploration-exploitation trade-off parameters.\n",
    "        \"\"\"\n",
    "        super().__init__(casino)\n",
    "        self.c = c\n",
    "        self.total_plays = [1 for _ in range(casino.number_of_bandits)]\n",
    "\n",
    "    def _policy(self):\n",
    "        \"\"\"\n",
    "        Determine the bandit selected for a round of play using UCB.\n",
    "\n",
    "        Returns:\n",
    "            {int} -- The index of the selected bandt.\n",
    "        \"\"\"\n",
    "        # Compute the \"best\" agent depending on the average win value and the number of time the bandit was selected.\n",
    "        # Based on the UCB formula.\n",
    "        ucb_values = [\n",
    "            self.gains[i] / self.total_plays[i] +\n",
    "            self.c * math.sqrt(math.log(max(1, self.number_of_turns)) / max(1, self.total_plays[i]))\n",
    "            for i in range(self.casino.number_of_bandits)\n",
    "        ]\n",
    "        return ucb_values.index(max(ucb_values))\n",
    "\n",
    "    def select(self):\n",
    "        \"\"\"\n",
    "        Activate the UCB bandit selection policy, play the selected bandit, and update rewards.\n",
    "\n",
    "        Returns:\n",
    "            {float} -- The gain obtained from playing the selected bandit.\n",
    "        \"\"\"\n",
    "        bandit_index = self._policy()\n",
    "        reward = super().select()              # Call the base method.\n",
    "        self.total_plays[bandit_index] += 1    # Edit to collect the number of time the bandit was pressed for each bandit.\n",
    "        return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCB Agent with c=1: Total Rewards = 4889.795662644114\n",
      "UCB Agent with c=2: Total Rewards = 4903.450085576625\n",
      "UCB Agent with c=3: Total Rewards = 4769.009455779889\n",
      "UCB Agent with c=4: Total Rewards = 4791.858126119577\n",
      "UCB Agent with c=5: Total Rewards = 4915.879716832812\n",
      "UCB Agent with c=6: Total Rewards = 4752.485164404493\n",
      "UCB Agent with c=7: Total Rewards = 4762.659381722687\n",
      "UCB Agent with c=8: Total Rewards = 4858.622923490334\n",
      "UCB Agent with c=9: Total Rewards = 4789.880436342095\n",
      "UCB Agent with c=10: Total Rewards = 4559.001273767392\n",
      "UCB Agent with c=11: Total Rewards = 4773.0366545064335\n",
      "UCB Agent with c=12: Total Rewards = 4718.054482629464\n",
      "UCB Agent with c=13: Total Rewards = 4667.231137189211\n",
      "UCB Agent with c=14: Total Rewards = 4384.900214110032\n",
      "UCB Agent with c=15: Total Rewards = 4328.0954355154845\n",
      "UCB Agent with c=16: Total Rewards = 4529.02078019051\n",
      "UCB Agent with c=17: Total Rewards = 4264.075427651087\n",
      "UCB Agent with c=18: Total Rewards = 4085.8380587541037\n",
      "UCB Agent with c=19: Total Rewards = 4190.9024285307505\n"
     ]
    }
   ],
   "source": [
    "# Create a casino with bandits.\n",
    "bandit1 = Bandit(2, 3)\n",
    "bandit2 = Bandit(-1, 6)\n",
    "bandit3 = Bandit(-2, 3)\n",
    "bandit4 = Bandit(5, 3)\n",
    "\n",
    "casino = Casino([bandit1, bandit2, bandit3, bandit4])\n",
    "total_turns = 1000\n",
    "results = []\n",
    "c_values = [x for x in range(1, 20, 1)]\n",
    "\n",
    "# Simulate actions for c values.\n",
    "for c in c_values:\n",
    "    agent_ucb = UCB_Agent(casino, c)\n",
    "    \n",
    "    # Play the bandits for a certain number of rounds.\n",
    "    for _ in range(total_turns):\n",
    "        agent_ucb.select()\n",
    "    \n",
    "    # Store the results.\n",
    "    results.append(agent_ucb.rewards)\n",
    "\n",
    "# Print the results.\n",
    "for i, c in enumerate(c_values):\n",
    "    print(f\"UCB Agent with c={c}: Total Rewards = {results[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** the UCB Agent's total rewards demonstrate a varyng pattern based on different values of the exploration-exploitation trade-off parameter (c). While increasing values of c generally lead to higher total rewards, there is a point where further exploration negatively impacts overall performance. The optimal value for c appears to be around 5, where the agent achieves the highest total rewards. Beyond this point, the agent's performance tends to plateau or decrease."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Quels sont les défauts des strategies dites gloutonnes ?\n",
    "2. Quel est l’avantage de l’algorithme UCB ?\n",
    "3. Quel est l’impact du paramètre c dans la formule d’estimation de valeur d’une action ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Downsides of Greedy Strategies:**\n",
    "   - **Overlooking Potential Gems:** Greedy strategies tend to stick with what seems best at the moment without exploring other options. This might cause us to miss out on actions that could be more profitable in the long run.\n",
    "   - **Stuck in the Comfort Zone:** Imagine if our strategy only focus on what seems locally optimal. We might get stuck in a comfortable routine and miss out on discovering better solutions.  \n",
    "\n",
    "2. **Why UCB Rocks:**\n",
    "   - **Finding the Sweet Spot:** The UCB algorithm strikes a cool balance between trying out new things and sticking with what has worked before. It considers how uncertain we re about each action, so we keep exploring while still favoring actions that have proven promising.  \n",
    "\n",
    "3. **What’s up with the 'c' in UCB:**\n",
    "   - **Dance Between Exploration and Exploitation :** The 'c' parameter in the UCB formula is like a dance partner, it decides how much we want to try new moves versus stikcing with the ones we know. A higher 'c' means we're more adventurous, but too high might lead to a messy dance. Finding the right 'c' is like adjusting the rhythm for the perfect balance between trying new things and sticking to what we know. Thi is the parameter that controls the trade-off between exploration and exploitation. The optimal choice of c depends on the problem and needs to be adjusted experimentally to find the right balance between exploration and exploitation, like we have do just before with our loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
